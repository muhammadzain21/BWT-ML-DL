{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Loading Data:\n",
    "   - Load the datasets.\n",
    "   - Display the first few rows of each dataset to understand its structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "used_cars_df = pd.read_csv(cars_file_path)\n",
    "weather_df=pd.read_csv(weather_file_path)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"--------------Used Cars Data--------------\")\n",
    "print(used_cars_df.head())\n",
    "\n",
    "print(\"--------Weather Classfication data--------\")\n",
    "print(weather_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________\n",
    "2. Handling Missing Values:\n",
    "   - Identify and handle missing values in both datasets. Provide a justification for the methods used to handle missing data.\n",
    "\n",
    "  ----- pakwheel used car-----\n",
    "In Pakwheels used car data there is a lotof missing values in assembly column so i put Local in the missing values because if we drop the missing values before adding Local to Assembly our data will becomes small\n",
    "\n",
    "After this we drop the missing value which is not to much\n",
    "\n",
    "-----Weather Classification data-----\n",
    "In this data we do not have to much missing values so we simply drop them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fill_Assembly(used_cars_df):\n",
    "    used_cars_df['assembly'].fillna('Local', inplace=True)\n",
    "    return used_cars_df\n",
    "\n",
    "def Drop_Missing_value(used_cars_df, weather_df):\n",
    "    used_cars_df = used_cars_df.dropna()\n",
    "    weather_df = weather_df.dropna()\n",
    "    return used_cars_df, weather_df\n",
    "\n",
    "used_cars_df = Fill_Assembly(used_cars_df)\n",
    "used_cars_df, weather_df = Drop_Missing_value(used_cars_df, weather_df)\n",
    "\n",
    "num_rows_car = used_cars_df.shape[0]\n",
    "print(f\"Number of rows in used_cars_df: {num_rows_car}\")\n",
    "\n",
    "num_rows_weather = weather_df.shape[0]\n",
    "print(f\"Number of rows in weather_df: {num_rows_weather}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________________________________________________________________________________________________\n",
    "3. Data Transformation:\n",
    "   - Convert categorical variables to numerical variables using appropriate encoding techniques (e.g., one-hot encoding, label encoding).\n",
    "   - Normalize/standardize the numerical features.\n",
    "\n",
    "\n",
    "------pakwheel used cars data---------\n",
    "\n",
    "In fuel_type, transmission and assembly column i use one hot encoding because values are not to many in these categorical values and on the others value i simply apply label encoding\n",
    "\n",
    "-----Weather Classification data------\n",
    "\n",
    "In this data i applied One hot encoding to every category column because the values in it is not to much\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_car_df = pd.get_dummies(used_cars_df, columns=['fuel_type'])\n",
    "used_car_df = pd.get_dummies(used_car_df, columns=['transmission'])\n",
    "used_car_df = pd.get_dummies(used_car_df, columns=['assembly'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "used_car_df['body'] = label_encoder.fit_transform(used_car_df['body'])\n",
    "used_car_df['ad_city'] = label_encoder.fit_transform(used_car_df['ad_city'])\n",
    "used_car_df['color'] = label_encoder.fit_transform(used_car_df['color'])\n",
    "used_car_df['make'] = label_encoder.fit_transform(used_car_df['make'])\n",
    "used_car_df['model'] = label_encoder.fit_transform(used_car_df['model'])\n",
    "used_car_df['registered'] = label_encoder.fit_transform(used_car_df['registered'])\n",
    "\n",
    "print(\"--------------Used Cars Data--------------\")\n",
    "print(used_car_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "used_cars_normalized = pd.DataFrame(scaler.fit_transform(used_car_df), columns=used_car_df.columns)\n",
    "print(used_cars_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "used_cars_standardized = pd.DataFrame(scaler.fit_transform(used_cars_normalized), columns=used_cars_normalized.columns)\n",
    "print(used_cars_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weathers_df=pd.get_dummies(weather_df,columns=['Cloud Cover'])\n",
    "weathers_df=pd.get_dummies(weathers_df,columns=['Season'])\n",
    "weathers_df=pd.get_dummies(weathers_df,columns=['Location'])\n",
    "weathers_df=pd.get_dummies(weathers_df,columns=['Weather Type'])\n",
    "\n",
    "\n",
    "print(\"--------Weather Classfication data--------\")\n",
    "print(weathers_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "weather_normalized = pd.DataFrame(scaler.fit_transform(weathers_df), columns=weathers_df.columns)\n",
    "print(weather_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Data Splitting:\n",
    "   - Split the data into training and testing sets using an 80-20 split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(used_cars_standardized, test_size=0.2)\n",
    "train2, test2 = train_test_split(weather_normalized, test_size=0.2)\n",
    "num_rows_train_car = train1.shape[0]\n",
    "print(\"------Pakwheel Used Data---------\\n\")\n",
    "print(f\"Number of rows of Training of Pakwheel data : {num_rows_train_car}\")  \n",
    "num_rows_test_car = test1.shape[0]\n",
    "print(f\"Number of rows of Testing of Pakwheel data : {num_rows_test_car}\\n\")  \n",
    "print(\"------Weather Classification data---------\\n\")\n",
    "num_rows_train_weather = train2.shape[0]\n",
    "print(f\"Number of rows of Trainig of Weather data: {num_rows_train_weather}\")  \n",
    "num_rows_test_weather = test2.shape[0]\n",
    "print(f\"Number of rows of Testing of Weather data: {num_rows_test_weather}\")  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
